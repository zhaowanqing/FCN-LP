{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfff377-de2c-450b-8790-68fccfa2f016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "from model import MLP, GCN, FCN_LP\n",
    "import random\n",
    "\n",
    "args = {}\n",
    "args['seed'] = 42\n",
    "args['device'] = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "args['hidden'] = 32\n",
    "args['num_classes'] = 2\n",
    "args['dropout'] = 0.5\n",
    "args['lpaiters'] = 1\n",
    "args['gcnnum'] = 4\n",
    "args['epochs'] = 500\n",
    "args['lr'] = 5e-5\n",
    "args['weight_decay'] = 5e-4\n",
    "def setup_seed(an_int):\n",
    "    np.random.seed(an_int)\n",
    "    random.seed(an_int)\n",
    "    torch.manual_seed(an_int)\n",
    "    torch.cuda.manual_seed(an_int)\n",
    "    torch.cuda.manual_seed_all(an_int)\n",
    "    torch.backends.cudnn.deterministic =True\n",
    "setup_seed(args['seed'])\n",
    "dataset_name = 'twitter' # twitter or weibo or pheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c325ab9-2982-4217-95c8-3fbb830c439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "# Load data and embeddings\n",
    "train_data = pd.read_csv('dataset/' + dataset_name + '/dataforGCN_train.csv')\n",
    "test_data = pd.read_csv('dataset/' + dataset_name + '/dataforGCN_test.csv')\n",
    "tweet_embeds = torch.load('dataset/' + dataset_name + '/ALLCAT_embeds_cross95.pt')\n",
    "tweet_graph = torch.load('dataset/' + dataset_name + '/edge_cross95.pt')\n",
    "\n",
    "# label to onehot vector\n",
    "label_list_train = train_data[\"label\"].tolist()\n",
    "event_list_train = train_data[\"event\"].tolist()\n",
    "\n",
    "label_list_test = test_data[\"label\"].tolist()\n",
    "event_list_test = test_data[\"event\"].tolist()\n",
    "\n",
    "labels = []\n",
    "for i, label_list in enumerate([label_list_train, label_list_test]):\n",
    "    labels_i = torch.zeros([len(label_list), 2], requires_grad=False)\n",
    "    for j, label in enumerate(label_list):\n",
    "        labels_i[j] = torch.FloatTensor([1.0, 0.0]) if label == 1 else torch.FloatTensor([0.0, 1.0])\n",
    "    labels.append(labels_i)\n",
    "\n",
    "labels = torch.cat(labels, 0)\n",
    "# Create data object\n",
    "data = Data(\n",
    "    x=tweet_embeds.float(),\n",
    "    edge_index=tweet_graph.coalesce().indices(),\n",
    "    edge_attr=tweet_graph.coalesce().values().unsqueeze(-1),\n",
    "    train_mask=torch.tensor([True]*len(label_list_train) + [False]*(len(labels)-len(label_list_train))).bool(),\n",
    "    test_mask=torch.tensor([False]*len(label_list_train) + [True]*(len(labels)-len(label_list_train))).bool(),\n",
    "    y=labels\n",
    ").to(args['device'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d10996-a44f-47f9-a675-95ee7f5cdaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting seen and unseen tweet\n",
    "def get_data_splits(label_list_train, event_list_train, selected_events, unselected_events):\n",
    "    event_map = {}\n",
    "    for i, (label, event) in enumerate(zip(label_list_train, event_list_train)):\n",
    "        if event in event_map:\n",
    "            event_map[event][0].append(i) if label == 1 else event_map[event][1].append(i)\n",
    "        else:\n",
    "            event_map[event]= [[],[]]\n",
    "            event_map[event][0].append(i) if label == 1 else event_map[event][1].append(i)\n",
    "\n",
    "    seen_real, seen_fake, unseen_real, unseen_fake = [], [], [], []\n",
    "    for event in selected_events:\n",
    "        seen_real.extend(event_map[event][0])\n",
    "        seen_fake.extend(event_map[event][1])\n",
    "    for event in unselected_events:\n",
    "        unseen_real.extend(event_map[event][0])\n",
    "        unseen_fake.extend(event_map[event][1])\n",
    "\n",
    "    return seen_real, seen_fake, unseen_real, unseen_fake\n",
    "\n",
    "\n",
    "if dataset_name == 'weibo':\n",
    "    all_tweets = set(range(0, len(label_list_train)))\n",
    "    unseen = set(random.sample(all_tweets,  len(label_list_train)// 3))\n",
    "    seen = list(all_tweets - unseen)\n",
    "    seen_real = [idx for idx in seen if label_list_train[idx] == 1]\n",
    "    seen_fake = [idx for idx in seen if label_list_train[idx] == 0]\n",
    "    unseen_real = [idx for idx in unseen if label_list_train[idx] == 1]\n",
    "    unseen_fake = [idx for idx in unseen if label_list_train[idx] == 0]    \n",
    "if dataset_name == 'twitter':\n",
    "    selected_events = ['boston','columbianChemicals', 'nepal',  'pigFish', 'bringback', 'sochi', 'malaysia', 'sandy', 'passport', 'underwater', 'livr']\n",
    "    unselected_events = ['elephant', 'garissa', 'eclipse', 'samurai']\n",
    "    seen_real, seen_fake, unseen_real, unseen_fake = get_data_splits(label_list_train, event_list_train, selected_events, unselected_events)\n",
    "    seen = seen_real + seen_fake\n",
    "if dataset_name == 'pheme':\n",
    "    selected_events = ['Ottawa Shooting', 'sydney siege', 'Charlie Hebdo', 'GermanwingsCrash']\n",
    "    unselected_events = ['Ferguson']\n",
    "    seen_real, seen_fake, unseen_real, unseen_fake = get_data_splits(label_list_train, event_list_train, selected_events, unselected_events)\n",
    "    seen = seen_real + seen_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea47a01-157a-4ccc-8632-9e28732b1503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmd import MMDLoss\n",
    "def accuracy(output, labels):\n",
    "    if output.ndim == 1:\n",
    "        output = torch.where(output>=0.5, 1, 0) \n",
    "        correct = (output == labels).sum()\n",
    "        accuracy = correct / len(labels)\n",
    "    else:\n",
    "        preds = output.max(1)[1].type_as(labels)   \n",
    "        labels = labels.max(1)[1]\n",
    "        correct = preds.eq(labels).double()\n",
    "        correct = correct.sum()\n",
    "        tp = torch.sum(preds * labels)\n",
    "        fp = torch.sum(preds * (1 - labels))\n",
    "        fn = torch.sum((1 - preds) * labels)\n",
    "        tn = torch.sum((1 - preds) * (1 - labels))\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        precision = tp / (tp + fp + 1e-10) \n",
    "        recall = tp / (tp + fn + 1e-10) \n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-10)\n",
    "    return accuracy, precision, recall, f1\n",
    "#model = GCN(tweet_embeds.shape[1], args['hidden'], args['num_classes'], args['dropout']).to(args['device'])\n",
    "#model = MLP(tweet_embeds.shape[1], args['hidden'], args['num_classes'], args['dropout']).to(args['device'])\n",
    "model = FCN_LP(tweet_embeds.shape[1], args['hidden'], args['num_classes'], args['dropout'], data.num_edges,\n",
    "                args['lpaiters'], args['gcnnum']).to(args['device'])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "crition1 = nn.CrossEntropyLoss() \n",
    "crition2 = MMDLoss(kernel_type = 'linear')\n",
    "\n",
    "best_score = 0\n",
    "\n",
    "for epoch in range(args['epochs']):\n",
    "    with torch.autograd.set_detect_anomaly(True):\n",
    "        t = time.time()\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out, yhat, x = model(data)\n",
    "        FCN_loss = crition1(out[seen], data.y[seen])\n",
    "        LPN_loss = crition1(yhat[seen], data.y[seen])\n",
    "        MMD_loss = crition2(x[unseen_real], x[seen_real]) + crition2(x[unseen_fake], x[seen_fake]) \n",
    "        loss_train = FCN_loss + LPN_loss + MMD_loss      \n",
    "        acc_train, precision, recall, f1 = accuracy(yhat[data.train_mask], data.y[data.train_mask])   \n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    out, yhat, x = model(data)\n",
    "    acc_test, precision_test, recall_test, f1_test = accuracy(yhat[data.test_mask], data.y[data.test_mask])\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "\n",
    "        print('Epoch: {:04d}'.format(epoch + 1),\\\n",
    "              'loss_train: {:.4f}'.format(loss_train.item()),\\\n",
    "              'acc_train: {:.4f}'.format(acc_train.item()),\\\n",
    "              'acc_test: {:.4f}'.format(acc_test.item()),\\\n",
    "              'time: {:.4f}s'.format(time.time() - t))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
